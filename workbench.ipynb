{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739f4abd-b539-44c1-ba83-045839181078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 06:36:51.297866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470491a5-7bc3-40d2-8936-865c154fc237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: GPU access blocked by the operating system\n",
      "Failed to properly shut down NVML: GPU access blocked by the operating system\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS, tf.float16]\n",
    "\n",
    "# Set the input and output tensors to the converter\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "converter.experimental_new_converter = True\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3bbdae-01d8-49ae-884c-36eb66012f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_512x512_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_1024x1024_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_hg104_1024x1024_kpts_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v1_fpn_512x512_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet101_v1_fpn_512x512_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v2_512x512_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v2_512x512_kpts_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20210210/centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20210210/centernet_mobilenetv2fpn_512x512_coco17_kpts.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d1_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d2_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d3_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d4_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d5_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d6_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d7_coco17_tpu-32.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_1024x1024_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet152_v1_fpn_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_1024x1024_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_800x1333_coco17_gpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_1024x1024_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_800x1333_coco17_gpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz\",\n",
    "    \"http://download.tensorflow.org/models/object_detection/tf2/20200711/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.tar.gz\",\n",
    "]\n",
    "print(len(models))\n",
    "pattern = r'(\\d+)x(\\d+)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "740cde3e-0d84-471d-87ef-da9e286b0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def download_model(model_url, path):\n",
    "    \"\"\"\n",
    "    Download a model from the given URL and save it to the specified path.\n",
    "\n",
    "    Args:\n",
    "    model_url (str): The URL of the model to download.\n",
    "    path (str): The path where the model will be saved.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "\n",
    "    # Send a GET request to download the model\n",
    "    print()\n",
    "    response = requests.get(model_url, stream=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        # Write the content of the response to the file\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    print(f\"Model downloaded successfully at {path}\")\n",
    "    return path\n",
    "def extract_tar(path):\n",
    "    \"\"\"\n",
    "    Extract a tar file to the specified path.\n",
    "\n",
    "    Args:\n",
    "    path (str): Path to the tar file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(path, \"r\") as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "    print(f\"Contents of {path} extracted to {path[:-7]}\")\n",
    "    return path[:-7]\n",
    "\n",
    "def saved_model_2_tflite(saved_model_dir, tflite_output_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "    tflite_model = converter.convert()\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS, tf.float16]\n",
    "    # Set the input and output tensors to the converter\n",
    "    converter.experimental_new_converter = True\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    with open(tflite_output_path + \".tflite\", 'wb') as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d162e2-d0c7-41aa-b648-f2a1719ad454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model downloaded successfully at ./ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
      "Contents of ./ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz extracted to ./ssd_mobilenet_v2_320x320_coco17_tpu-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1716275323.123252      27 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716275323.123345      27 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-21 07:08:43.123607: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:08:43.203982: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-21 07:08:43.204029: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:08:43.849693: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-21 07:08:44.608737: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:08:45.058930: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 1935330 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model downloaded successfully at ./ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
      "Contents of ./ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz extracted to ./ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1716275348.550843      27 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716275348.550895      27 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-21 07:09:08.551090: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:08.617229: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-21 07:09:08.617265: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:09.209448: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-21 07:09:10.300314: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:11.038071: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 2486984 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model downloaded successfully at ./ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
      "Contents of ./ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz extracted to ./ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1716275373.824252      27 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716275373.824300      27 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-21 07:09:33.824485: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:33.894811: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-21 07:09:33.894845: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:34.555573: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-21 07:09:35.482546: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:35.974697: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 2150214 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model downloaded successfully at ./ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n",
      "Contents of ./ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz extracted to ./ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1716275394.230033      27 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716275394.230080      27 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-21 07:09:54.230266: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:54.305965: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-21 07:09:54.306000: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:54.991606: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-21 07:09:55.889923: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:09:56.408579: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 2178316 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model downloaded successfully at ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
      "Contents of ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz extracted to ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1716275433.803547      27 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716275433.803593      27 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-21 07:10:33.803795: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:10:33.874784: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-21 07:10:33.874818: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:10:34.562030: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-21 07:10:35.707279: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: ./ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model\n",
      "2024-05-21 07:10:36.238701: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 2434909 microseconds.\n"
     ]
    }
   ],
   "source": [
    "for i in range(19,24):\n",
    "    model_url = models[i]\n",
    "    model_tar_file = model_url.split(\"/\")[-1]\n",
    "    \n",
    "    downloaded_model_path = download_model(model_url, \"./\" + model_tar_file)\n",
    "    \n",
    "    extract_path = extract_tar(downloaded_model_path)\n",
    "    \n",
    "    if os.path.isfile(downloaded_model_path):\n",
    "        os.remove(downloaded_model_path)\n",
    "    \n",
    "    saved_model_2_tflite(extract_path + \"/saved_model\", \"tflite_models/\" + extract_path[2:])\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(extract_path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93e88c-21a6-41ee-bb22-fa04d5436dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c166ef0-51a5-4e0f-96aa-999d62a2fa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae8585bf-015a-4d9b-9f3f-65e4a1b71a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details:\n",
      "Name: serving_default_input_tensor:0\n",
      "Shape: [1 1 1 3]\n",
      "Type: <class 'numpy.uint8'>\n",
      "\n",
      "Output details:\n",
      "Name: StatefulPartitionedCall:6\n",
      "Shape: [    1 51150     4]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:0\n",
      "Shape: [1 1]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:5\n",
      "Shape: [1]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:7\n",
      "Shape: [    1 51150    91]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:1\n",
      "Shape: [1 1 1]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:2\n",
      "Shape: [1 1]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:4\n",
      "Shape: [1 1]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n",
      "Name: StatefulPartitionedCall:3\n",
      "Shape: [1 1 1]\n",
      "Type: <class 'numpy.float32'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tflite_model_info(model_path):\n",
    "    # Load the TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    print(\"Input details:\")\n",
    "    for detail in input_details:\n",
    "        print(\"Name:\", detail['name'])\n",
    "        print(\"Shape:\", detail['shape'])\n",
    "        print(\"Type:\", detail['dtype'])\n",
    "        print()\n",
    "\n",
    "    print(\"Output details:\")\n",
    "    for detail in output_details:\n",
    "        print(\"Name:\", detail['name'])\n",
    "        print(\"Shape:\", detail['shape'])\n",
    "        print(\"Type:\", detail['dtype'])\n",
    "        print()\n",
    "\n",
    "# Replace 'your_model_path.tflite' with the path to your TFLite model\n",
    "model_path = '/tf/Bolt/tflite_models/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tflite'\n",
    "print_tflite_model_info(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9332172a-ea70-48f2-882a-7381d8cb9733",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(interpreter\u001b[38;5;241m.\u001b[39mget_tensor_details())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Find tensors that are used during inference\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m used_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Filter out unused tensors and save the optimized model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optimized_tensors \u001b[38;5;241m=\u001b[39m [tensor \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mtensor_details \u001b[38;5;28;01mif\u001b[39;00m tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m used_tensors]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor indices\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "# Get input tensor details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "\n",
    "# Check the input type and create a dummy input accordingly\n",
    "if input_details['dtype'] == np.float32:\n",
    "    dummy_input = np.zeros(input_details['shape'], dtype=np.float32)\n",
    "elif input_details['dtype'] == np.uint8:\n",
    "    # If the input type is uint8, generate random values between 0 and 255\n",
    "    dummy_input = np.random.randint(0, 256, size=input_details['shape'], dtype=np.uint8)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported input type\")\n",
    "\n",
    "# Set the dummy input tensor\n",
    "interpreter.set_tensor(input_index, dummy_input)\n",
    "\n",
    "# Invoke inference to trace execution paths\n",
    "interpreter.invoke()\n",
    "n = len(interpreter.get_tensor_details())\n",
    "# Find tensors that are used during inference\n",
    "used_tensors = set(interpreter.tensor(i)() for i in range(n))\n",
    "\n",
    "# Filter out unused tensors and save the optimized model\n",
    "optimized_tensors = [tensor for tensor in interpreter.tensor_details if tensor['index'] in used_tensors]\n",
    "interpreter.resize_tensor_input(input_index, [1] + list(input_details['shape'][1:]))\n",
    "interpreter.resize_tensor_input(output_index, [1] + list(interpreter.get_output_details()[0]['shape'][1:]))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "for idx, tensor in enumerate(optimized_tensors):\n",
    "    interpreter.set_tensor(idx, interpreter.tensor(tensor['index'])())\n",
    "\n",
    "# Save the optimized model\n",
    "with open(model_path+\"_optimized_model.tflite\", \"wb\") as f:\n",
    "    f.write(interpreter.tensor_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9a21b-79d1-4e67-ab89-1e842e274b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
